{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60181e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from ast import literal_eval\n",
    "from openai import AzureOpenAI\n",
    "from functools import lru_cache\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman(x, y):\n",
    "    assert len(x) == len(y) > 0\n",
    "    q = lambda n: map(lambda val: sorted(n).index(val) + 1, n)\n",
    "    d = sum(map(lambda x, y: (x - y) ** 2, q(x), q(y)))\n",
    "    return 1.0 - 6.0 * d / float(len(x) * (len(y) ** 2 - 1.0))\n",
    "\n",
    "\n",
    "def sat_evaluation(pred, label, sat_num):\n",
    "    acc = sum([int(p == l) for p, l in zip(pred, label)]) / len(label)\n",
    "    precision = precision_score(label, pred, average='macro', zero_division=0)\n",
    "    sk_recall = recall_score(label, pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(label, pred, average='macro', zero_division=0)\n",
    "    #     sat_result = (acc, precision, sk_recall, f1)\n",
    "\n",
    "    recall = [[0, 0] for _ in range(sat_num)]\n",
    "    for p, l in zip(pred, label):\n",
    "        recall[l][1] += 1\n",
    "        recall[l][0] += int(p == l)\n",
    "    recall_value = [item[0] / max(item[1], 1) for item in recall]\n",
    "\n",
    "    UAR = sum(recall_value) / len(recall_value)\n",
    "    kappa = cohen_kappa_score(pred, label)\n",
    "    rho = spearman(pred, label)\n",
    "\n",
    "    bi_pred = [int(item < sat_num // 2) for item in pred]\n",
    "    bi_label = [int(item < sat_num // 2) for item in label]\n",
    "    bi_recall = sum([int(p == l) for p, l in zip(bi_pred, bi_label) if l == 1]) / max(bi_label.count(1), 1)\n",
    "    bi_precision = sum([int(p == l) for p, l in zip(bi_pred, bi_label) if p == 1]) / max(bi_pred.count(1), 1)\n",
    "    bi_f1 = 2 * bi_recall * bi_precision / max((bi_recall + bi_precision), 1)\n",
    "\n",
    "    sat_result = [UAR, kappa, rho, bi_f1, acc, precision, sk_recall, f1]\n",
    "    return sat_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    dirname = f'dataset/{dataset_name}'\n",
    "    print(\"Reading\", dataset_name, \"dataset\")\n",
    "    \n",
    "    result = dict()\n",
    "    for set_name in ['train', 'valid', 'test']:\n",
    "        total_conversations = 0\n",
    "        data_list = list()\n",
    "        with open(os.path.join(dirname, f'{set_name}_{dataset_name}.txt'), 'r', encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                items = line.strip('\\n').split('\\t')\n",
    "                input_text = eval(items[0])\n",
    "                sat = int(items[2])\n",
    "                history = ''\n",
    "                for text in input_text:\n",
    "                    user_utt = text.split('|||')[0]\n",
    "                    ai_utt = text.split('|||')[1]\n",
    "                    if ai_utt:\n",
    "                        history += f'\\n\\nUser: {user_utt}'\n",
    "                        history += f'\\n\\nAssistant: {ai_utt}'\n",
    "\n",
    "                if len(user_utt.strip()) > 0 and user_utt.strip() != 'OVERALL':\n",
    "                    data_list.append({\n",
    "                        'history': history.strip(),\n",
    "                        'utterance': user_utt.strip(),\n",
    "                        'full_conv': f'{history.strip()}\\n\\nHuman: {user_utt.strip()}',\n",
    "                        'label': sat\n",
    "                    })\n",
    "                elif user_utt.strip() == 'OVERALL':\n",
    "                    total_conversations += 1\n",
    "\n",
    "            result[set_name] = pd.DataFrame(data_list)\n",
    "        \n",
    "        print('{} set, len: {} utterances, {} conversations'.format(set_name, len(result[set_name]), total_conversations))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ded776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "mwoz_data = load_data('mwoz')\n",
    "sgd_data = load_data('sgd')\n",
    "redial_data = load_data('redial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccede197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label distribution\n",
    "for name, dataset in [('mwoz', mwoz_data), ('sgd', sgd_data), ('redial', redial_data)]:\n",
    "    total_examples = 0\n",
    "    total_sat = 0\n",
    "    total_neu = 0\n",
    "    total_dis = 0\n",
    "    for set_name in ['train', 'valid', 'test']:\n",
    "        set_data = dataset[set_name]\n",
    "        total_examples += len(set_data)\n",
    "        total_sat += len(set_data[set_data['label'] == 2])\n",
    "        total_neu += len(set_data[set_data['label'] == 1])\n",
    "        total_dis += len(set_data[set_data['label'] == 0])\n",
    "    print(f'ratios for {name}')\n",
    "    print(\"{0:.1%}, {1:.1%}, {2:.1%}\".format(total_sat/total_examples, total_neu/total_examples, total_dis/total_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbef6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_output_file = \"results.csv\"\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"COMPLETION_ENDPOINT_URL\")\n",
    "api_version = os.getenv(\"COMPLETION_API_VERSION\")\n",
    "\n",
    "model_name = \"gpt-4.1-mini\"\n",
    "\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "completion_client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c201f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_completion(messages, temperature, max_tokens):\n",
    "    try:\n",
    "        completion = completion_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        return completion\n",
    "    except Exception as e:\n",
    "        print(\"FAILED COMPLETION: \")\n",
    "        print(e)\n",
    "        print(messages)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf24ae9",
   "metadata": {},
   "source": [
    "# Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65797a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_SHOT_PROMPT = \"\"\"Your task is to assess the user's satisfaction at the end of the given conversation. Choose only one of these labels:\n",
    "- \"satisfied\" : The assistant's response successfully meets the user's needs and the user appears satisfied.\n",
    "- \"dissatisfied\" : The user's needs are not met and they appear dissatisfied.\n",
    "- \"neutral\" : Neither satisfied nor dissatisfied, or the conversation is purely informational, routine, or a greeting.\n",
    "\n",
    "Output only one word: \"satisfied\", \"dissatisfied\", or \"neutral\". Do not add any explanation or extra text.\n",
    "\n",
    "# conversation\n",
    "{full_conv}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_mapping = {\n",
    "    'satisfied': 2,\n",
    "    'neutral': 1,\n",
    "    'dissatisfied': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_zs(full_conv):\n",
    "    response = run_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": ZERO_SHOT_PROMPT.format(full_conv=full_conv)}]}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=5\n",
    "        )\n",
    "    return response_mapping.get(response.choices[0].message.content.strip(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "mwoz_data['test']['zs_pred'] = mwoz_data['test']['full_conv'].progress_apply(predict_zs)\n",
    "sat_evaluation(mwoz_data['test']['zs_pred'].values, mwoz_data['test']['label'].values, sat_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "sgd_data['test']['zs_pred'] = sgd_data['test']['full_conv'].progress_apply(predict_zs)\n",
    "sat_evaluation(sgd_data['test']['zs_pred'].values, sgd_data['test']['label'].values, sat_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1849fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "redial_data['test']['zs_pred'] = redial_data['test']['full_conv'].progress_apply(predict_zs)\n",
    "sat_evaluation(redial_data['test']['zs_pred'].values, redial_data['test']['label'].values, sat_num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c552af",
   "metadata": {},
   "source": [
    "# Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9657b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_examples(df_train):\n",
    "    pos_ex = df_train[df_train['label'] == 2].sample(n=1).iloc[0]\n",
    "    neu_ex = df_train[df_train['label'] == 1].sample(n=1).iloc[0]\n",
    "    neg_ex = df_train[df_train['label'] == 0].sample(n=1).iloc[0]\n",
    "    return f\"\"\"### Example 1\n",
    "#conversation\n",
    "{pos_ex['full_conv']}\n",
    "\n",
    "# answer\n",
    "satisfied\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2\n",
    "#conversation\n",
    "{neu_ex['full_conv']}\n",
    "\n",
    "# answer\n",
    "neutral\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3\n",
    "#conversation\n",
    "{neg_ex['full_conv']}\n",
    "\n",
    "# answer\n",
    "dissatisfied\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz_examples = get_formatted_examples(mwoz_data['train'])\n",
    "sgd_examples = get_formatted_examples(sgd_data['train'])\n",
    "redial_examples = get_formatted_examples(redial_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mwoz_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_PROMPT = \"\"\"Your task is to assess the user's satisfaction at the end of the given conversation. Choose only one of these labels:\n",
    "\n",
    "- \"satisfied\": The assistant's response successfully meets the user's needs and the user appears satisfied.\n",
    "- \"dissatisfied\": The user's needs are not met and they appear dissatisfied.\n",
    "- \"neutral\": Neither satisfied nor dissatisfied, or the conversation is purely informational, routine, or a greeting.\n",
    "\n",
    "Output only one word: \"satisfied\", \"dissatisfied\", or \"neutral\". Do not add any explanation or extra text.\n",
    "\n",
    "## Examples\n",
    "\n",
    "{examples}\n",
    "---\n",
    "\n",
    "\n",
    "## Now evaluate the following conversation\n",
    "\n",
    "# conversation\n",
    "{full_conv}\n",
    "\n",
    "# answer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fs(full_conv, data_examples):\n",
    "    response = run_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": FEW_SHOT_PROMPT.format(full_conv=full_conv, examples=data_examples)}]}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=5\n",
    "        )\n",
    "    return response_mapping.get(response.choices[0].message.content.strip(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "mwoz_data['test']['fs_pred'] = mwoz_data['test']['full_conv'].progress_apply(lambda conv: predict_fs(conv, mwoz_examples))\n",
    "sat_evaluation(mwoz_data['test']['fs_pred'].values, mwoz_data['test']['label'].values, sat_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "sgd_data['test']['fs_pred'] = sgd_data['test']['full_conv'].progress_apply(lambda conv: predict_fs(conv, sgd_examples))\n",
    "sat_evaluation(sgd_data['test']['fs_pred'].values, sgd_data['test']['label'].values, sat_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "redial_data['test']['fs_pred'] = redial_data['test']['full_conv'].progress_apply(lambda conv: predict_fs(conv, mwoz_examples))\n",
    "sat_evaluation(redial_data['test']['fs_pred'].values, redial_data['test']['label'].values, sat_num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0f71b",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d89a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz_data['test'].to_csv('zero-few-shot-predictions-mwoz.csv', index=False)\n",
    "sgd_data['test'].to_csv('zero-few-shot-predictions-sgd.csv', index=False)\n",
    "redial_data['test'].to_csv('zero-few-shot-predictions-redial.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f756b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
